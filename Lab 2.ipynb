{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DltCvAsdLSzQ"
      },
      "source": [
        "<h1 style=\"text-align:center;\"><b>Laboratorio 2</b></h1>\n",
        "<h3 style=\"text-align:center;\">Marcos Díaz (221102), Daniel Machic (22118), Maria Jose Ramírez (221051)</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMln11NnLcII"
      },
      "source": [
        "**Github**: https://github.com/mac2218/Seminario-Lab02"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0boGZajJLXQk"
      },
      "source": [
        "# Librerias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhVgF6PsLHGc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1--6MgUNzvU"
      },
      "source": [
        "# **Ejercicio 1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hDbmtGILIHA",
        "outputId": "c43743e9-63d5-4a0a-97a8-c2ca31327f8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Posteriores P(Y|X):\n",
            " [[0.375      0.69565217 0.62962963 0.19230769]\n",
            " [0.625      0.30434783 0.37037037 0.80769231]]\n",
            "\n",
            "Regla óptima (0=Y0, 1=Y1):\n",
            " [1 0 0 1]\n",
            "\n",
            "Error de Bayes: 0.31\n"
          ]
        }
      ],
      "source": [
        "# Probabilidades conjuntas P(X, Y)\n",
        "joint = np.array([\n",
        "    [0.09, 0.16, 0.17, 0.05],  # Y=0\n",
        "    [0.15, 0.07, 0.10, 0.21]   # Y=1\n",
        "])\n",
        "\n",
        "\n",
        "px = joint.sum(axis=0)\n",
        "\n",
        "\n",
        "posterior = joint / px\n",
        "\n",
        "\n",
        "decision = np.argmax(posterior, axis=0)\n",
        "\n",
        "# Error de Bayes\n",
        "error = sum(px[i] * np.min(posterior[:, i]) for i in range(len(px)))\n",
        "\n",
        "print(\"Posteriores P(Y|X):\\n\", posterior)\n",
        "print(\"\\nRegla óptima (0=Y0, 1=Y1):\\n\", decision)\n",
        "print(\"\\nError de Bayes:\", error)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYKUSKO7Lxvv"
      },
      "source": [
        "## Clasificador Bayesiano Óptimo\n",
        "\n",
        "### 1. Probabilidades posteriores\n",
        "\n",
        "A partir de la tabla de probabilidades conjuntas \\( P(X,Y) \\), calculamos:\n",
        "$P(Y \\mid X=x) = \\frac{P(X=x, Y)}{P(X=x)}$\n",
        "\n",
        "Las probabilidades posteriores obtenidas son:\n",
        "\n",
        "$\n",
        "P(Y \\mid X)$ =\n",
        "\\begin{bmatrix}\n",
        "0.375 & 0.6957 & 0.6296 & 0.1923 \\\\\n",
        "0.625 & 0.3043 & 0.3704 & 0.8077\n",
        "\\end{bmatrix}\n",
        "\n",
        "\n",
        "donde la primera fila corresponde a \\( Y=0 \\) y la segunda a \\( Y=1 \\).\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Regla de clasificación óptima\n",
        "\n",
        "El clasificador Bayesiano asigna la clase con mayor probabilidad posterior:\n",
        "\n",
        "$hat{y}(x) = \\arg\\max_{y \\in \\{0,1\\}} P(Y=y \\mid X=x)$\n",
        "\n",
        "Por lo tanto:\n",
        "\n",
        "$\n",
        "\\hat{y}(1)=1, \\quad\n",
        "\\hat{y}(2)=0, \\quad\n",
        "\\hat{y}(3)=0, \\quad\n",
        "\\hat{y}(4)=1\n",
        "$\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Error de Bayes\n",
        "\n",
        "El error mínimo de clasificación está dado por:\n",
        "\n",
        "$\n",
        "P_e = \\sum_{x} P(X=x)\\min \\{P(Y=0 \\mid X=x), P(Y=1 \\mid X=x)\\}\n",
        "$\n",
        "Sustituyendo valores:\n",
        "\n",
        "$\n",
        "P_e = 0.31\n",
        "$\n",
        "\n",
        "Por lo tanto, el error del clasificador Bayesiano óptimo es:\n",
        "\n",
        "$\n",
        "\\boxed{31\\%}\n",
        "$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ejercicio 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Punto de decisión (x*): 0.5493061443340549\n",
            "\n",
            "Regla de clasificación:\n",
            "Si x < 0.5493061443340549 → Y = 1\n",
            "Si x >= 0.5493061443340549 → Y = 0\n",
            "\n",
            "Regiones de clasificación:\n",
            "R1 = [0, 0.5493061443340549 )\n",
            "R0 = [ 0.5493061443340549 , ∞)\n",
            "\n",
            "Error del clasificador: 0.3075499102701248\n"
          ]
        }
      ],
      "source": [
        "# Punto de decisión Bayesiano\n",
        "x_star = np.log(3) / 2\n",
        "\n",
        "print(\"Punto de decisión (x*):\", x_star)\n",
        "\n",
        "# Regla de clasificación\n",
        "\n",
        "def clasificador_bayes(x):\n",
        "    \"\"\"\n",
        "    Regla Bayesiana óptima:\n",
        "    Clasificar como Y=1 si x < x*\n",
        "    Clasificar como Y=0 si x >= x*\n",
        "    \"\"\"\n",
        "    if x < x_star:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "print(\"\\nRegla de clasificación:\")\n",
        "print(\"Si x <\", x_star, \"→ Y = 1\")\n",
        "print(\"Si x >=\", x_star, \"→ Y = 0\")\n",
        "\n",
        "# Regiones de clasificación\n",
        "\n",
        "print(\"\\nRegiones de clasificación:\")\n",
        "print(\"R1 = [0,\", x_star, \")\")\n",
        "print(\"R0 = [\", x_star, \", ∞)\")\n",
        "\n",
        "# Cálculo del error\n",
        "# Error = 0.5 P(X∈R1|Y=0) + 0.5 P(X∈R0|Y=1)\n",
        "\n",
        "error = 0.5 * (1 - np.exp(-x_star)) + 0.5 * np.exp(-3 * x_star)\n",
        "\n",
        "print(\"\\nError del clasificador:\", error)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "El clasificador bayesiano óptimo para las distribuciones exponenciales consideradas establece un punto de decisión en $x^* \\approx 0.5493$. Esto significa que valores pequeños de la variable observada se asocian más probablemente a la clase $Y=1$ (distribución con parámetro 3), mientras que valores mayores se asignan a la clase $Y=0$ (distribución con parámetro 1). La razón es que la distribución con parámetro mayor decrece más rápidamente, concentrando mayor probabilidad en valores cercanos a cero. El error del clasificador es aproximadamente $0.308$, lo que indica que, aun siendo óptimo en el sentido bayesiano, existe un solapamiento considerable entre ambas distribuciones, generando una probabilidad no despreciable de clasificación incorrecta."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ejercicio 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Datos\n",
        "data = {\n",
        "    \"Outlook\": [\"Rainy\",\"Rainy\",\"Overcast\",\"Sunny\",\"Sunny\",\"Sunny\",\"Overcast\",\"Rainy\",\n",
        "                \"Rainy\",\"Sunny\",\"Rainy\",\"Overcast\",\"Overcast\",\"Sunny\"],\n",
        "    \"Temperature\": [\"Hot\",\"Hot\",\"Hot\",\"Mild\",\"Cool\",\"Cool\",\"Cool\",\"Mild\",\n",
        "                    \"Cool\",\"Mild\",\"Mild\",\"Mild\",\"Hot\",\"Mild\"],\n",
        "    \"Humidity\": [\"High\",\"High\",\"High\",\"High\",\"Normal\",\"Normal\",\"Normal\",\"High\",\n",
        "                 \"Normal\",\"Normal\",\"Normal\",\"High\",\"Normal\",\"High\"],\n",
        "    \"Windy\": [False,True,False,False,False,True,True,False,\n",
        "              False,False,True,True,False,True],\n",
        "    \"Play\": [\"No\",\"No\",\"Yes\",\"Yes\",\"Yes\",\"No\",\"Yes\",\"No\",\n",
        "             \"Yes\",\"Yes\",\"Yes\",\"Yes\",\"Yes\",\"No\"]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Codificar variables categóricas\n",
        "df_encoded = df.copy()\n",
        "\n",
        "encoders = {}\n",
        "for col in df.columns:\n",
        "    le = LabelEncoder()\n",
        "    df_encoded[col] = le.fit_transform(df[col])\n",
        "    encoders[col] = le\n",
        "\n",
        "X = df_encoded.drop(\"Play\", axis=1)\n",
        "y = df_encoded[\"Play\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicciones Naive Bayes: ['No' 'Yes']\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import CategoricalNB\n",
        "\n",
        "model_nb = CategoricalNB()\n",
        "model_nb.fit(X, y)\n",
        "\n",
        "# Casos a clasificar\n",
        "test = pd.DataFrame({\n",
        "    \"Outlook\": [\"Rainy\", \"Sunny\"],\n",
        "    \"Temperature\": [\"Hot\", \"Hot\"],\n",
        "    \"Humidity\": [\"High\", \"Normal\"],\n",
        "    \"Windy\": [False, False]\n",
        "})\n",
        "\n",
        "# Codificar test usando mismos encoders\n",
        "test_encoded = test.copy()\n",
        "for col in test.columns:\n",
        "    test_encoded[col] = encoders[col].transform(test[col])\n",
        "\n",
        "pred_nb = model_nb.predict(test_encoded)\n",
        "\n",
        "print(\"Predicciones Naive Bayes:\", encoders[\"Play\"].inverse_transform(pred_nb))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicciones KNN: ['No' 'Yes']\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "model_knn = KNeighborsClassifier(n_neighbors=3)\n",
        "model_knn.fit(X, y)\n",
        "\n",
        "pred_knn = model_knn.predict(test_encoded)\n",
        "\n",
        "print(\"Predicciones KNN:\", encoders[\"Play\"].inverse_transform(pred_knn))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Métricas de desempeño"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== NAIVE BAYES ===\n",
            "Accuracy: 0.9285714285714286\n",
            "Precision: 0.9\n",
            "Recall: 1.0\n",
            "F1 Score: 0.9473684210526315\n",
            "ROC AUC: 0.9222222222222223\n",
            "\n",
            "=== KNN ===\n",
            "Accuracy: 0.8571428571428571\n",
            "Precision: 0.8888888888888888\n",
            "Recall: 0.8888888888888888\n",
            "F1 Score: 0.8888888888888888\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "# Predicciones sobre el entrenamiento (dataset pequeño)\n",
        "y_pred_nb = model_nb.predict(X)\n",
        "y_pred_knn = model_knn.predict(X)\n",
        "\n",
        "print(\"=== NAIVE BAYES ===\")\n",
        "print(\"Accuracy:\", accuracy_score(y, y_pred_nb))\n",
        "print(\"Precision:\", precision_score(y, y_pred_nb))\n",
        "print(\"Recall:\", recall_score(y, y_pred_nb))\n",
        "print(\"F1 Score:\", f1_score(y, y_pred_nb))\n",
        "print(\"ROC AUC:\", roc_auc_score(y, model_nb.predict_proba(X)[:,1]))\n",
        "\n",
        "print(\"\\n=== KNN ===\")\n",
        "print(\"Accuracy:\", accuracy_score(y, y_pred_knn))\n",
        "print(\"Precision:\", precision_score(y, y_pred_knn))\n",
        "print(\"Recall:\", recall_score(y, y_pred_knn))\n",
        "print(\"F1 Score:\", f1_score(y, y_pred_knn))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En el conjunto de datos “Play Golf”, ambos clasificadores producen las mismas predicciones para los casos solicitados. Sin embargo, al evaluar las métricas de desempeño, Naive Bayes presenta mejores resultados generales, con mayor Accuracy, F1 Score y ROC AUC. En particular, alcanza un Recall de 1.0, lo que indica que identifica correctamente todas las instancias positivas. Esto se debe a que Naive Bayes es especialmente adecuado para variables categóricas y conjuntos de datos pequeños, mientras que KNN puede verse afectado por la codificación numérica de variables categóricas y la elección del parámetro k. Por tanto, en este problema, Naive Bayes muestra un desempeño superior.\n",
        "\n",
        "## **¿Cómo construir un clasificador bueno con tan pocos datos?**\n",
        "\n",
        "Cuando se dispone de un conjunto de datos pequeño, como en este caso (14 observaciones), es fundamental utilizar modelos simples que tengan baja varianza y menor riesgo de sobreajuste. Algoritmos como Naive Bayes o Regresión Logística suelen ser más adecuados que modelos complejos, ya que requieren menos datos para estimar sus parámetros de manera estable.\n",
        "\n",
        "Además, es importante aplicar técnicas como validación cruzada para obtener una estimación más confiable del desempeño, dado que separar en entrenamiento y prueba puede reducir demasiado el tamaño efectivo del conjunto de datos. También resulta clave incorporar conocimiento previo del problema y realizar una correcta codificación de variables. En general, con pocos datos, la simplicidad del modelo y la correcta validación son más importantes que la complejidad del algoritmo."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (Inteligencia Artificial)",
      "language": "python",
      "name": "ia_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
